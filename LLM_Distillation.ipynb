{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Distilling Step by Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Student Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the teacher (LLM) and student model\n",
    "STUDENT_MODEL = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(STUDENT_MODEL)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since sst2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/simranjeetsingh1497/.cache/huggingface/datasets/sst2/default/0.0.0/8d51e7e4887a4caaa95b3fbebbf53c0490b58bbb (last modified on Thu Feb 27 14:11:45 2025).\n"
     ]
    }
   ],
   "source": [
    "# Load a small Hugging Face dataset (Change this to your preferred dataset)\n",
    "dataset = load_dataset(\"sst2\", split=\"train[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEACHER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain AI in one sentence. Hmm, where do I start? Let me think about what AI is. AI stands for Artificial Intelligence, right? It's like a machine that can perform tasks that typically require human intelligence. So maybe something like \"Artificial Intelligence enables machines to perform tasks that typically require human intelligence.\" That sounds good, but let me check if it covers all aspects.\n",
      "\n",
      "Wait, does it cover things like learning, problem-solving, creativity? Yeah, AI uses various techniques to learn from data and make decisions or take actions without being explicitly programmed. So the sentence should mention that AI allows machines to learn, adapt, and perform tasks that require human-like reasoning.\n",
      "\n",
      "I think I got it. The key points are: AI enables machines to do things like learning, problem-solving, creativity, and decision-making. So putting it all together in one sentence would be something like \"Artificial Intelligence enables machines to perform tasks that typically require human intelligence.\" That seems concise and covers the main idea.\n",
      "\n",
      "I don't think I need any more details because the question is just asking for a single explanation. So this should work.\n",
      "</think>\n",
      "\n",
      "Artificial Intelligence enables machines to perform tasks that typically require human intelligence.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the chat model\n",
    "llm_engine = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "\n",
    "def generate_rationale(input_text):\n",
    "    \"\"\"\n",
    "    Uses Ollama's DeepSeek R1 model to generate a step-by-step rationale for the given input.\n",
    "    \"\"\"\n",
    "    prompt = f\"Explain step-by-step reasoning before answering: {input_text}\"\n",
    "    \n",
    "    response = llm_engine.invoke(prompt)  # Using LangChain's invoke method\n",
    "    \n",
    "    return response.content if hasattr(response, \"content\") else response\n",
    "\n",
    "print(generate_rationale(\"Explain AI in one sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_data at 0x2ae1ae9d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 10/10 [08:26<00:00, 50.66s/ examples]\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset with rationales\n",
    "def process_data(example):\n",
    "    input_text = example[\"sentence\"]  # Change this depending on your dataset format\n",
    "    rationale = generate_rationale(input_text)\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    # Tokenize input and rationale\n",
    "    input_enc = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=256)\n",
    "    rationale_enc = tokenizer(rationale, truncation=True, padding=\"max_length\", max_length=256)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"labels\": label,\n",
    "        \"rationale_ids\": rationale_enc[\"input_ids\"],\n",
    "        \"rationale_mask\": rationale_enc[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# Apply function to dataset\n",
    "processed_dataset = dataset.map(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask', 'labels', 'rationale_ids', 'rationale_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 564.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming 'dataset' is your Dataset object\n",
    "processed_dataset.save_to_disk('preprocessed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the saved directory\n",
    "processed_dataset = load_from_disk('preprocessed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask', 'labels', 'rationale_ids', 'rationale_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save the model and checkpoints\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        rationale_ids = inputs.pop(\"rationale_ids\", None)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        label_loss = loss_fn(outputs.logits, labels)\n",
    "        \n",
    "        if rationale_ids is not None:\n",
    "            rationale_outputs = model(input_ids=rationale_ids, attention_mask=inputs[\"attention_mask\"])\n",
    "            rationale_loss = loss_fn(rationale_outputs.logits, rationale_ids)\n",
    "            loss = label_loss + 0.5 * rationale_loss  # Weighted loss\n",
    "        else:\n",
    "            loss = label_loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    eval_dataset=processed_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask', 'labels', 'rationale_ids', 'rationale_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./results\")\n",
    "print(\"✅ Distillation Complete! Smaller model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For AutoTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "data = {\n",
    "    \"text\": [],\n",
    "    \"rationale\": [],\n",
    "    \"target\": []\n",
    "}\n",
    "\n",
    "for example in dataset:\n",
    "    input_text = example[\"sentence\"]\n",
    "    label = example[\"label\"]\n",
    "    rationale = generate_rationale(input_text)\n",
    "    \n",
    "    data[\"text\"].append(input_text)\n",
    "    data[\"rationale\"].append(rationale)\n",
    "    data[\"target\"].append(label)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.21 ('new_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e01d37aa128af6d3d62f071bf1f9af7ef23b3d03ef1427c96c926c15be542b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
